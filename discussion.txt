2.3 Experiments and discussion.

(1). How might you improve the classification accuracy of the Gaussian mixtures,
without adding more training data?

For improving the classification accuracy of the Gaussian mixtures,
(a) Increasing the value of max_iter, we gain the better classification accuracy of Gaussian mixtures .
(b) Increasing component count.




(2). When would your classifier decide that a given test utterance comes from
none of the trained speaker models, and how would your classifier come to this decision?

(a) loglik_threshold
(b)





(3). Can you think of some alternative methods for doing speaker identification
 that donâ€™t use Gaussian mixtures?

3.1 Proportion of correctly identified phonemes
-----------------------------------------------

Using a the default HMM with 8 gaussian mixtures, 3 states, all 30 examples of
training data, all 14 dimensions of the data, and a maximum iteration limit of
20, the number of correctly identified phonemes was found to be 492/1096 or 44.89%.

3.2 Analysis of various HMMs
-------------------------------

train size       mixtures        states      max_iter        dimensions      correct/total

    30              8               3           20              14              492/1096
    30              5               3           20              14
    30              2               3           20              14
    30              1               3           20              14

    30              8               3           3               14
    24              8               3           3               14
    16              8               3           3               14
    8               8               3           3               14

    30              8               3           20              10              483/1096
    30              8               3           20              7               402/1096
    30              8               3           20              3               260/1096
    30              8               3           20              1               116/1096

    30              8               3           5               14              485/1096
    30              8               3           3               14              506/1096
    30              8               2           5               14              511/1096
    30              8               1           5               14              488/1096

3.3 Levenshtein distance
------------------------

In this section we measured the Levenshtein distance between the transcriptions
from hypothesis.txt, and the reference sentences unkn_1.txt - unkn_30.txt, from
the speechdata/Testing directory. Th values for the entire data set were SE =
0.134375, IE = 0.034375, DE = 0.040625, and LEV_DIST = 0.209375.

4.1 Report
----------

Using the IBM Watson speech recognition platform, the .flac files from
speechdata/Testing were transcribed, and the results were placed in a new
hypothesis file. The Levenshtein algorithm was run on them to calculate the
Levenshtein distance of the results, and the following results were reported:
